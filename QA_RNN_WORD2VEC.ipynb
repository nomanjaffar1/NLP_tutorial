{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBgnUjkH59tM",
        "outputId": "bb34260b-fccd-429b-81dc-5ca01836f366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pypdf import PdfReader\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "k6FHx_3o6FOr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read the PDF and extract text\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from all pages of a PDF file.\n",
        "\n",
        "    Args:\n",
        "    pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    str: Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    for page_num in range(len(reader.pages)):\n",
        "      text += reader.pages[page_num].extract_text()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "57py54nj6FRt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the Text\n",
        "def preprocess_text(corpus):\n",
        "    \"\"\"\n",
        "    Tokenizes the corpus and pads the sequences.\n",
        "\n",
        "    Args:\n",
        "    corpus (list of str): List of sentences.\n",
        "\n",
        "    Returns:\n",
        "    tuple: tokenizer (Tokenizer object), padded_sequences (np.array), word_index (dict), max_len (int)\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    sequences = tokenizer.texts_to_sequences(corpus)\n",
        "    word_index = tokenizer.word_index\n",
        "    max_len = max(len(seq) for seq in sequences)\n",
        "\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "    return tokenizer, padded_sequences, word_index, max_len"
      ],
      "metadata": {
        "id": "4d9ZJ7ce6FUt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create the Word2Vec Model\n",
        "def train_word2vec_model(corpus_tokens, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Trains a Word2Vec model on the tokenized corpus.\n",
        "\n",
        "    Args:\n",
        "    corpus_tokens (list of list of str): Tokenized sentences.\n",
        "    embedding_dim (int): Dimension of the word vectors.\n",
        "\n",
        "    Returns:\n",
        "    Word2Vec: Trained Word2Vec model.\n",
        "    \"\"\"\n",
        "    word2vec_model = Word2Vec(corpus_tokens, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "    return word2vec_model"
      ],
      "metadata": {
        "id": "PX4Fr-OM6FX5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create the Embedding Matrix\n",
        "def create_embedding_matrix(word_index, word2vec_model, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Creates an embedding matrix from the Word2Vec model.\n",
        "\n",
        "    Args:\n",
        "    word_index (dict): Dictionary mapping words to their integer index.\n",
        "    word2vec_model (Word2Vec): Trained Word2Vec model.\n",
        "    embedding_dim (int): Dimension of the word vectors.\n",
        "\n",
        "    Returns:\n",
        "    tuple: embedding_matrix (np.array), vocab_size (int)\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "    return embedding_matrix, vocab_size"
      ],
      "metadata": {
        "id": "-vDqCiYs6Fbi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build the RNN Model\n",
        "def build_rnn_model(vocab_size, embedding_dim, max_len, embedding_matrix):\n",
        "    \"\"\"\n",
        "    Builds the RNN model for next word prediction.\n",
        "\n",
        "    Args:\n",
        "    vocab_size (int): Size of the vocabulary.\n",
        "    embedding_dim (int): Dimension of the word vectors.\n",
        "    max_len (int): Maximum length of the input sequences.\n",
        "    embedding_matrix (np.array): Embedding matrix.\n",
        "\n",
        "    Returns:\n",
        "    Sequential: Compiled RNN model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len-1, trainable=False),\n",
        "        LSTM(64),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "fbgsJueS6FeJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Create Input-Output Pairs for Next Word Prediction\n",
        "def create_sequences(tokenizer, corpus, max_len):\n",
        "    \"\"\"\n",
        "    Creates input-output pairs for next word prediction.\n",
        "\n",
        "    Args:\n",
        "    tokenizer (Tokenizer): Fitted tokenizer.\n",
        "    corpus (list of str): List of sentences.\n",
        "    max_len (int): Maximum length of the input sequences.\n",
        "\n",
        "    Returns:\n",
        "    tuple: input_sequences (np.array), output_words (np.array)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            sequences.append(n_gram_sequence)\n",
        "\n",
        "    sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "    input_sequences, output_words = sequences[:,:-1], sequences[:,-1]\n",
        "    return input_sequences, output_words"
      ],
      "metadata": {
        "id": "YxlnsMgs6Fg8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "pdf_path = '/content/2407.12220v1.pdf'  # Replace with your actual PDF file path\n",
        "\n",
        "# Extract text from the PDF\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Split the text into sentences for processing\n",
        "corpus = text.split('.')\n",
        "\n",
        "# Tokenize the corpus for Word2Vec\n",
        "corpus_tokens = [sentence.lower().split() for sentence in corpus]\n",
        "\n",
        "# Preprocess the text\n",
        "tokenizer, padded_sequences, word_index, max_len = preprocess_text(corpus)\n",
        "\n",
        "# Train the Word2Vec model\n",
        "word2vec_model = train_word2vec_model(corpus_tokens)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_matrix, vocab_size = create_embedding_matrix(word_index, word2vec_model)\n",
        "\n",
        "# Build the RNN model\n",
        "model = build_rnn_model(vocab_size, 300, max_len, embedding_matrix)\n",
        "\n",
        "# Create input-output pairs for next word prediction\n",
        "input_sequences, output_words = create_sequences(tokenizer, corpus, max_len)\n",
        "output_words = np.array(output_words).reshape(-1, 1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# The model is now ready for training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR4snpf96FkZ",
        "outputId": "25fea1a3-6e55-466c-e73f-0b6d5fdac592"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 174, 300)          1743000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                93440     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5810)              377650    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2214090 (8.45 MB)\n",
            "Trainable params: 471090 (1.80 MB)\n",
            "Non-trainable params: 1743000 (6.65 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(input_sequences, output_words, epochs=30, batch_size=32)  # Uncomment and add target data for training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUvMU_PM9dsV",
        "outputId": "c6075afc-f390-4e93-b279-56bab06c9d5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "709/709 [==============================] - 114s 158ms/step - loss: 7.5833\n",
            "Epoch 2/30\n",
            "709/709 [==============================] - 113s 159ms/step - loss: 7.1064\n",
            "Epoch 3/30\n",
            "709/709 [==============================] - 111s 156ms/step - loss: 6.9180\n",
            "Epoch 4/30\n",
            "709/709 [==============================] - 110s 156ms/step - loss: 6.7747\n",
            "Epoch 5/30\n",
            "709/709 [==============================] - 111s 156ms/step - loss: 6.6412\n",
            "Epoch 6/30\n",
            "709/709 [==============================] - 111s 157ms/step - loss: 6.5800\n",
            "Epoch 7/30\n",
            "709/709 [==============================] - 111s 157ms/step - loss: 6.4917\n",
            "Epoch 8/30\n",
            "709/709 [==============================] - 112s 158ms/step - loss: 6.3611\n",
            "Epoch 9/30\n",
            "709/709 [==============================] - 111s 156ms/step - loss: 6.3076\n",
            "Epoch 10/30\n",
            "709/709 [==============================] - 109s 154ms/step - loss: 6.1867\n",
            "Epoch 11/30\n",
            "709/709 [==============================] - 111s 156ms/step - loss: 6.1063\n",
            "Epoch 12/30\n",
            "709/709 [==============================] - 110s 156ms/step - loss: 6.0047\n",
            "Epoch 13/30\n",
            "709/709 [==============================] - 110s 155ms/step - loss: 5.9152\n",
            "Epoch 14/30\n",
            "709/709 [==============================] - 112s 159ms/step - loss: 5.8269\n",
            "Epoch 15/30\n",
            "709/709 [==============================] - 111s 156ms/step - loss: 5.9614\n",
            "Epoch 16/30\n",
            "709/709 [==============================] - 109s 154ms/step - loss: 6.0124\n",
            "Epoch 17/30\n",
            "709/709 [==============================] - 107s 151ms/step - loss: 5.8142\n",
            "Epoch 18/30\n",
            "709/709 [==============================] - 107s 152ms/step - loss: 5.6926\n",
            "Epoch 19/30\n",
            "709/709 [==============================] - 107s 151ms/step - loss: 5.6044\n",
            "Epoch 20/30\n",
            "709/709 [==============================] - 109s 154ms/step - loss: 5.5354\n",
            "Epoch 21/30\n",
            "709/709 [==============================] - 109s 154ms/step - loss: 5.4813\n",
            "Epoch 22/30\n",
            "709/709 [==============================] - 110s 155ms/step - loss: 5.4170\n",
            "Epoch 23/30\n",
            "709/709 [==============================] - 109s 154ms/step - loss: 5.3630\n",
            "Epoch 24/30\n",
            "709/709 [==============================] - 111s 157ms/step - loss: 5.3085\n",
            "Epoch 25/30\n",
            "709/709 [==============================] - 110s 155ms/step - loss: 5.2583\n",
            "Epoch 26/30\n",
            "709/709 [==============================] - 110s 155ms/step - loss: 5.2187\n",
            "Epoch 27/30\n",
            "709/709 [==============================] - 110s 155ms/step - loss: 5.1524\n",
            "Epoch 28/30\n",
            "709/709 [==============================] - 112s 158ms/step - loss: 5.1038\n",
            "Epoch 29/30\n",
            "709/709 [==============================] - 110s 155ms/step - loss: 5.0553\n",
            "Epoch 30/30\n",
            "709/709 [==============================] - 110s 156ms/step - loss: 5.0056\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ccf4c4d0100>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_next_word(model, tokenizer, text, max_len):\n",
        "    \"\"\"\n",
        "    Predicts the next word for a given text using the trained model.\n",
        "\n",
        "    Args:\n",
        "    model (Sequential): Trained RNN model.\n",
        "    tokenizer (Tokenizer): Fitted tokenizer.\n",
        "    text (str): Input text for prediction.\n",
        "    max_len (int): Maximum length of input sequences.\n",
        "\n",
        "    Returns:\n",
        "    str: Predicted next word.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "    # Pad the token list\n",
        "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
        "\n",
        "    # Predict the next word\n",
        "    predicted_probabilities = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted_probabilities, axis=-1)[0]\n",
        "\n",
        "    # Convert the predicted word index to the actual word\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_len, num_words):\n",
        "    \"\"\"\n",
        "    Generates text by predicting the next word for a given seed text.\n",
        "\n",
        "    Args:\n",
        "    model (Sequential): Trained RNN model.\n",
        "    tokenizer (Tokenizer): Fitted tokenizer.\n",
        "    seed_text (str): Initial text to start the prediction.\n",
        "    max_len (int): Maximum length of input sequences.\n",
        "    num_words (int): Number of words to generate.\n",
        "\n",
        "    Returns:\n",
        "    str: Generated text.\n",
        "    \"\"\"\n",
        "    text = seed_text\n",
        "    for _ in range(num_words):\n",
        "        next_word = predict_next_word(model, tokenizer, text, max_len)\n",
        "        text += \" \" + next_word\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \" While the most contamination is likely to happen at pre/post training\"\n",
        "generated_text = generate_text(model, tokenizer, seed_text, max_len, 10)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-fkaLQM9fBG",
        "outputId": "7c696364-9aa5-4ebf-b2f0-91d71ac611e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " While the most contamination is likely to happen at pre/post training al the test set is the test set is the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0M8LVfQ9e7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "idbir2vY9e1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeIyln3P9ex9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrwHfFVV9euq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJm9rP289ert"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}