{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##importing the necessary labraries"
      ],
      "metadata": {
        "id": "MVqPceX3yzoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opcaFsmR0GGW",
        "outputId": "0606a8ce-b54d-4059-bf92-2e29dd4cca0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "qpuisNZyIVor"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read the pdf text file"
      ],
      "metadata": {
        "id": "Ba6oUC4XzP0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required classes\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# creating a pdf reader object\n",
        "reader = PdfReader('/content/2407.12220v1.pdf')"
      ],
      "metadata": {
        "id": "l-FSyx2zEqGV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing number of pages in pdf file\n",
        "print(len(reader.pages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIk1jUHnE0Er",
        "outputId": "36e6a690-222a-4780-bc03-45b69bd6cf8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a page object\n",
        "page = reader.pages[0]"
      ],
      "metadata": {
        "id": "7OdZ1ml5EvAs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting text from page\n",
        "# print(page.extract_text())\n",
        "reader.pages[12].extract_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "BhPlaCv3AlC5",
        "outputId": "09a68b6f-09a7-4b78-ec7c-83bcbdb8e700"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'text fragments, with extreme label noise [Zellers et al., 2019b]) is often rounded off to “a test of (general)\\ncommonsense reasoning” [Gemini et al., 2023, Edwards, 2023, Wang and Zhao, 2023].\\nMore subjective but less artificial and gameable tests are being explored. The LMSYS Arena [Chiang et al.,\\n2024] (which crowdsources binary human preferences between models on arbitrary prompts) solves many\\nproblems – though we note that it too is susceptible to hacking (for instance by improving the style of a model\\nwithout improving accuracy or reasoning, or even by paying raters to score your model preferentially using\\ntell-tale tokens).\\n3.2.5 Subset hacking: picking the easy part of a hard task\\nA more subtle degree of freedom than simply the choice of training dataset arises because model evaluation\\nhas become very costly due to the number of benchmarks available, dataset sizes, and the associated inference\\ncosts (see Liang et al. [2022] for a detailed discussion). A common solution for smaller labs is to use a subset\\nof the benchmark to estimate the full score. But the choice of subset is a degree of freedom that can give rise\\nto hacking.\\nIn addition to just choosing a subset on which your method performs better, subsetting a test set (for example,\\nthe MATH dataset of Hendrycks et al. [2021b]), allows for several more subtle forms of hacking, especially if\\ncomparing against performances measured by other papers on the full dataset:\\n1. Re-generating subsets until an easier (by level, or subject) set of questions is found;\\n2.Stratifying problems by difficulty, then sampling more from the easier levels (easy for benchmarks\\nlike MATH where the difficulties are given [Lightman et al., 2023]);\\n3.Testing on the full benchmark with high k(that is, e.g. 50 samples per test data entry), then\\nconstructing a subset with a representative difficulty distribution using only those hard questions that\\nyour model can solve.\\nKapoor et al. [2024] mentions several papers which subsetted a benchmark during evaluation, which may not\\nnecessarily constitute subset hacking . For example, Shinn et al. [2024] did not report removing three problems\\nfrom HumanEval, and Sodhi et al. [2023] evaluated on a subset excluding 8 problems from WebArena.\\nReporting all seeds used for dataset generation could be one way to increase transparency, yet does not fully\\nsolve the problem. Preregistering does not necessarily solve the problem as we do not know if the resulting\\nsubset distributions will be representative. The most transparent solution is probably for the benchmark\\ndeveloper to provide ‘official’ subsets, when possible.\\n3.2.6 Harness hacking: choosing evaluation details after test\\nWe usually default to assuming that results on a given benchmark are comparable, but this is not always true.\\nIt is possible to create or choose evaluation code (‘harness’) which favours your model [Fourrier et al., 2023].\\nThis is a consequence of overlooking the implementation details of benchmarks, and of evaluation loops in\\nparticular.\\nA clear example of this occurred following the release of Falcon-40b, when the OpenLLM leaderboard\\nevaluated Llama-65b’s MMLU score as significantly lower than those published in the LLaMA paper\\n[Fourrier et al., 2023], and lower than models such as Falcon-40b. The OpenLLM leaderboard used the\\nEleutherAI LM Evaluation Harness. This contrasts with two other implementations, namely the original\\nharness from the MMLU paper [Hendrycks et al., 2020] and the HELM [Liang et al., 2022] implementation,\\nwhich scored Llama-65b nearly 30% higher than its Eleuther result [Fourrier et al., 2023].\\nThe difference between the three prompts in the three harnesses were extremely subtle:\\n13'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\n",
        "for page_num in range(len(reader.pages)):\n",
        "  # print(reader.pages[page_num].extract_text())\n",
        "  text += reader.pages[page_num].extract_text()"
      ],
      "metadata": {
        "id": "B_-LVK9KEb8a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preprocess the data"
      ],
      "metadata": {
        "id": "ZsUWTBHhzYOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "f_PFarb-IM69"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "w3JzRMEVAlPo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(tokenizer.word_index)+1\n",
        "# tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "4ngjQrOOIrRN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in text.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "i9-tLt9_IrU1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts(input_sequences[80:81])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qUly8h4KM1j",
        "outputId": "f2030ce4-db3f-4b75-b03a-26c585ddc255"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['language models llms on public']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "oJmfPct4Kw0W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')"
      ],
      "metadata": {
        "id": "fiGI9-efM4sw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ADya20M4vV",
        "outputId": "5c2b3092-1272-400d-84e5-00585eeba24e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0, 274, 151],\n",
              "       [  0,   0,   0, ..., 274, 151,   6],\n",
              "       [  0,   0,   0, ..., 151,   6,  75],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 492,  31, 307],\n",
              "       [  0,   0,   0, ...,  31, 307, 223],\n",
              "       [  0,   0,   0, ..., 307, 223,  60]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]\n",
        "\n",
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "7asWaVxVM4y2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyk-MI-XM5k0",
        "outputId": "c2d9ccd5-b274-48ab-fea9-164a5eb78019"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23309, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = to_categorical(y,num_classes=num_classes)"
      ],
      "metadata": {
        "id": "yXXjtyLhM5oY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL TRAINING"
      ],
      "metadata": {
        "id": "5lpGhRUUzdgc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO9mT61_M51d"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(num_classes, 100,input_length = max_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(num_classes,activation= 'softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqBEQnIJjrVN",
        "outputId": "a420b7d7-85b3-4cdf-c80d-9cff166db314"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 31, 100)           581000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5810)              877310    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1608910 (6.14 MB)\n",
            "Trainable params: 1608910 (6.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B72hEQgPmKSC",
        "outputId": "f12c6827-fda1-40b6-e9b4-f1881d43d906"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.2274 - accuracy: 0.9522\n",
            "Epoch 2/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.2117 - accuracy: 0.9532\n",
            "Epoch 3/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1943 - accuracy: 0.9555\n",
            "Epoch 4/40\n",
            "729/729 [==============================] - 56s 76ms/step - loss: 0.1833 - accuracy: 0.9567\n",
            "Epoch 5/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1768 - accuracy: 0.9557\n",
            "Epoch 6/40\n",
            "729/729 [==============================] - 58s 80ms/step - loss: 0.1681 - accuracy: 0.9567\n",
            "Epoch 7/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1618 - accuracy: 0.9577\n",
            "Epoch 8/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1574 - accuracy: 0.9568\n",
            "Epoch 9/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1518 - accuracy: 0.9572\n",
            "Epoch 10/40\n",
            "729/729 [==============================] - 58s 79ms/step - loss: 0.1500 - accuracy: 0.9569\n",
            "Epoch 11/40\n",
            "729/729 [==============================] - 57s 79ms/step - loss: 0.1450 - accuracy: 0.9570\n",
            "Epoch 12/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1423 - accuracy: 0.9569\n",
            "Epoch 13/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1406 - accuracy: 0.9576\n",
            "Epoch 14/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1388 - accuracy: 0.9573\n",
            "Epoch 15/40\n",
            "729/729 [==============================] - 59s 80ms/step - loss: 0.1393 - accuracy: 0.9571\n",
            "Epoch 16/40\n",
            "729/729 [==============================] - 57s 79ms/step - loss: 0.1328 - accuracy: 0.9580\n",
            "Epoch 17/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1315 - accuracy: 0.9577\n",
            "Epoch 18/40\n",
            "729/729 [==============================] - 58s 79ms/step - loss: 0.1295 - accuracy: 0.9577\n",
            "Epoch 19/40\n",
            "729/729 [==============================] - 58s 80ms/step - loss: 0.1282 - accuracy: 0.9574\n",
            "Epoch 20/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1279 - accuracy: 0.9574\n",
            "Epoch 21/40\n",
            "729/729 [==============================] - 57s 79ms/step - loss: 0.1284 - accuracy: 0.9576\n",
            "Epoch 22/40\n",
            "729/729 [==============================] - 59s 80ms/step - loss: 0.1321 - accuracy: 0.9568\n",
            "Epoch 23/40\n",
            "729/729 [==============================] - 58s 80ms/step - loss: 0.1286 - accuracy: 0.9575\n",
            "Epoch 24/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1242 - accuracy: 0.9568\n",
            "Epoch 25/40\n",
            "729/729 [==============================] - 62s 85ms/step - loss: 0.1210 - accuracy: 0.9581\n",
            "Epoch 26/40\n",
            "729/729 [==============================] - 60s 82ms/step - loss: 0.1202 - accuracy: 0.9579\n",
            "Epoch 27/40\n",
            "729/729 [==============================] - 60s 83ms/step - loss: 0.1204 - accuracy: 0.9578\n",
            "Epoch 28/40\n",
            "729/729 [==============================] - 58s 79ms/step - loss: 0.1229 - accuracy: 0.9576\n",
            "Epoch 29/40\n",
            "729/729 [==============================] - 58s 79ms/step - loss: 0.1254 - accuracy: 0.9567\n",
            "Epoch 30/40\n",
            "729/729 [==============================] - 57s 79ms/step - loss: 0.1233 - accuracy: 0.9576\n",
            "Epoch 31/40\n",
            "729/729 [==============================] - 58s 79ms/step - loss: 0.1207 - accuracy: 0.9568\n",
            "Epoch 32/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1167 - accuracy: 0.9588\n",
            "Epoch 33/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1167 - accuracy: 0.9580\n",
            "Epoch 34/40\n",
            "729/729 [==============================] - 58s 80ms/step - loss: 0.1174 - accuracy: 0.9579\n",
            "Epoch 35/40\n",
            "729/729 [==============================] - 58s 79ms/step - loss: 0.1208 - accuracy: 0.9568\n",
            "Epoch 36/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1274 - accuracy: 0.9556\n",
            "Epoch 37/40\n",
            "729/729 [==============================] - 59s 81ms/step - loss: 0.1284 - accuracy: 0.9551\n",
            "Epoch 38/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1162 - accuracy: 0.9576\n",
            "Epoch 39/40\n",
            "729/729 [==============================] - 57s 78ms/step - loss: 0.1134 - accuracy: 0.9587\n",
            "Epoch 40/40\n",
            "729/729 [==============================] - 56s 77ms/step - loss: 0.1133 - accuracy: 0.9580\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c76c426cf70>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREDICT THE NEXT WORDS"
      ],
      "metadata": {
        "id": "fIEboA6Xzjwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text = \" While the most contamination is likely to happen at pre/post training\"\n",
        "import numpy as np\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=max_len-1, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLRgoqNVmOF_",
        "outputId": "77080bbf-9c8d-4760-df80-7e650ab6ba0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            " While the most contamination is likely to happen at pre/post training it\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common to\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common to use\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common to use multiple\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common to use multiple relevant\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common to use multiple relevant for\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            " While the most contamination is likely to happen at pre/post training it is now common to use multiple relevant for liao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQp2RMi33r4m",
        "outputId": "350a6dbe-0d40-4b8c-ff1b-607680a93623"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}